{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(X, W):\n",
    "    H = np.dot(X, W)\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(H):\n",
    "    p = 1 / (1 + np.exp(-H))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0.9996646498695336\n"
     ]
    }
   ],
   "source": [
    "H = linear(X, W)\n",
    "print(H)\n",
    "p = sigmoid(H)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([-4, -3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10\n",
      "4.53978687024e-05\n"
     ]
    }
   ],
   "source": [
    "H = linear(X, W)\n",
    "print(H)\n",
    "p = sigmoid(H)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"data\"]\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 4.6,  3.4,  1.4,  0.3],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W'] = 0.0001 * np.random.randn(4, 3)\n",
    "        self.params['b'] = np.ones(3)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        W = self.params['W']\n",
    "        b = self.params['b']\n",
    "\n",
    "        h = np.dot(X, W) + b\n",
    "        a = np.exp(h)\n",
    "        #stable_a = np.exp(h - np.max(h, axis = 1).reshape(-1,1))\n",
    "        p = a/np.sum(a, axis = 1).reshape(-1,1)\n",
    "        \n",
    "        return p\n",
    "    \n",
    "    def loss(self, X, T):\n",
    "        \n",
    "        p = self.forward(X)\n",
    "        \n",
    "        n = T.shape[0]\n",
    "        log_likelihood = 0\n",
    "        log_likelihood -= np.log(p[np.arange(n), T]).sum()\n",
    "        Loss = log_likelihood / n\n",
    "\n",
    "        return Loss\n",
    "    \n",
    "    def accuracy(self, X, T):\n",
    "        p = self.forward(X) #예측\n",
    "        predict = np.argmax(p, axis = 1) #예측 결과 index 1darray 로 출력 \n",
    "        \n",
    "        return 1 - np.count_nonzero(predict - T)/len(T)\n",
    "        \n",
    "    def gradient(self, X, T, learning_rate = 0.0001):\n",
    "        \n",
    "        p = self.forward(X)\n",
    "        \n",
    "        t = np.zeros((T.shape[0], 3))\n",
    "        t[np.arange(T.shape[0]), T] = 1\n",
    "        #t는 인덱스 레이블 T를 One hot 벡터로 바꾼 것\n",
    "        \n",
    "        dp = p.copy()\n",
    "        dp[np.arange(len(T)), T] -= 1\n",
    "        dp /= len(T)\n",
    "        \n",
    "        #목적함수에 대한 가중치 미분값을 담을 zero array 생성\n",
    "        grads = {}\n",
    "        grads['W'] = np.zeros((4, 3))\n",
    "        grads['b'] = np.zeros(10)\n",
    "        #목적함수에 대한 가중치 미분값 합 구하기\n",
    "        grads['W'] = np.dot(X.T, (p-t)/len(T))\n",
    "        grads['b'] = np.sum((p-t)/len(T), axis = 0)\n",
    "        #p-t 대신 dp 사용 가능\n",
    "        \n",
    "        self.params['W'] -= learning_rate * grads['W']\n",
    "        self.params['b'] -= learning_rate * grads['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 학습중입니다.\n",
      "Accuracy :  0.6666666666666667\n",
      "Loss :      0.837258789697\n",
      "[[ 0.04489043 -0.00302755 -0.04177102]\n",
      " [ 0.13593987 -0.05260227 -0.08335439]\n",
      " [-0.21403781  0.06099198  0.15326508]\n",
      " [-0.09858452  0.01258931  0.08602048]]\n",
      "1000 번째 학습중입니다.\n",
      "Accuracy :  0.6666666666666667\n",
      "Loss :      0.804785146931\n",
      "[[ 0.05509918 -0.0032708  -0.05173652]\n",
      " [ 0.15963724 -0.06165687 -0.09799717]\n",
      " [-0.24691491  0.07084157  0.17629261]\n",
      " [-0.11392999  0.01406223  0.09989303]]\n",
      "2000 번째 학습중입니다.\n",
      "Accuracy :  0.6666666666666667\n",
      "Loss :      0.776127383534\n",
      "[[ 0.06448324 -0.00321122 -0.06118015]\n",
      " [ 0.18178077 -0.07015501 -0.11164256]\n",
      " [-0.27785428  0.08007122  0.19800232]\n",
      " [-0.12834075  0.01524657  0.11311946]]\n",
      "3000 번째 학습중입니다.\n",
      "Accuracy :  0.6666666666666667\n",
      "Loss :      0.750718923069\n",
      "[[ 0.07314435 -0.00288925 -0.07016323]\n",
      " [ 0.20252399 -0.07813882 -0.12440196]\n",
      " [-0.30701452  0.08870293  0.21853085]\n",
      " [-0.14189688  0.01615592  0.12576624]]\n",
      "4000 번째 학습중입니다.\n",
      "Accuracy :  0.6666666666666667\n",
      "Loss :      0.728079564684\n",
      "[[ 0.08117179 -0.00234023 -0.0787397 ]\n",
      " [ 0.22200687 -0.08564953 -0.13637413]\n",
      " [-0.33454591  0.09676507  0.2380001 ]\n",
      " [-0.15467353  0.01680617  0.13789262]]\n",
      "5000 번째 학습중입니다.\n",
      "Accuracy :  0.6799999999999999\n",
      "Loss :      0.707805275156\n",
      "[[ 0.0886425  -0.00159437 -0.08695626]\n",
      " [ 0.24035499 -0.09272599 -0.14764579]\n",
      " [-0.3605885   0.10428963  0.25651813]\n",
      " [-0.16674008  0.01721429  0.14955106]]\n",
      "6000 번째 학습중입니다.\n",
      "Accuracy :  0.6799999999999999\n",
      "Loss :      0.689557339839\n",
      "[[ 0.09562241 -0.00067729 -0.09485325]\n",
      " [ 0.25768003 -0.09940417 -0.15829265]\n",
      " [-0.38527103  0.11131003  0.27418026]\n",
      " [-0.1781599   0.01739743  0.16078774]]\n",
      "7000 번째 학습중입니다.\n",
      "Accuracy :  0.6799999999999999\n",
      "Loss :      0.673052101258\n",
      "[[  1.02168031e-01   3.89248304e-04  -1.02465413e-01]\n",
      " [  2.74080927e-01  -1.05716981e-01  -1.68380741e-01]\n",
      " [ -4.08710678e-01   1.17859655e-01   2.91070284e-01]\n",
      " [ -1.88990271e-01   1.73723412e-02   1.71643198e-01]]\n",
      "8000 번째 학습중입니다.\n",
      "Accuracy :  0.6799999999999999\n",
      "Loss :      0.658051830243\n",
      "[[ 0.10832789  0.0015867  -0.10982272]\n",
      " [ 0.28964516 -0.11169435 -0.1779676 ]\n",
      " [-0.43101348  0.12397091  0.30726183]\n",
      " [-0.19928281  0.01715503  0.18215305]]\n",
      "9000 번째 학습중입니다.\n",
      "Accuracy :  0.7\n",
      "Loss :      0.644356889223\n",
      "[[ 0.11414375  0.00289919 -0.11695107]\n",
      " [ 0.30445002 -0.11736339 -0.18710343]\n",
      " [-0.45227497  0.12967462  0.32281961]\n",
      " [-0.2090838   0.01676054  0.19234853]]\n",
      "10000 번째 학습중입니다.\n",
      "Accuracy :  0.7066666666666667\n",
      "Loss :      0.631799148747\n",
      "[[ 0.11965167  0.0043131  -0.1238729 ]\n",
      " [ 0.31856389 -0.12274858 -0.1958321 ]\n",
      " [-0.47258109  0.13499979  0.33780056]\n",
      " [-0.21843476  0.01620288  0.20225714]]\n",
      "11000 번째 학습중입니다.\n",
      "Accuracy :  0.7333333333333334\n",
      "Loss :      0.620236529051\n",
      "[[ 0.12488291  0.00581665 -0.13060769]\n",
      " [ 0.33204729 -0.12787203 -0.20419205]\n",
      " [-0.49200906  0.13997342  0.3522549 ]\n",
      " [-0.22737283  0.01549502  0.21190308]]\n",
      "12000 번째 학습중입니다.\n",
      "Accuracy :  0.76\n",
      "Loss :      0.609548509123\n",
      "[[ 0.12986465  0.00739963 -0.13717242]\n",
      " [ 0.34495395 -0.13275368 -0.21221706]\n",
      " [-0.51062828  0.14462053  0.36622701]\n",
      " [-0.23593134  0.01464888  0.22130773]]\n",
      "13000 번째 학습중입니다.\n",
      "Accuracy :  0.76\n",
      "Loss :      0.599632446781\n",
      "[[ 0.13462065  0.00905313 -0.14358192]\n",
      " [ 0.35733163 -0.13741157 -0.21993685]\n",
      " [-0.52850115  0.14896421  0.3797562 ]\n",
      " [-0.24414015  0.01367539  0.23049003]]\n",
      "14000 번째 학습중입니다.\n",
      "Accuracy :  0.78\n",
      "Loss :      0.590400568075\n",
      "[[ 0.13917173  0.01076936 -0.14984922]\n",
      " [ 0.3692229  -0.14186197 -0.22737773]\n",
      " [-0.54568387  0.15302569  0.39287744]\n",
      " [-0.25202611  0.01258458  0.2394668 ]]\n",
      "15000 번째 학습중입니다.\n",
      "Accuracy :  0.7866666666666666\n",
      "Loss :      0.581777503958\n",
      "[[ 0.14353618  0.01254146 -0.15598577]\n",
      " [ 0.38066583 -0.14611963 -0.23456299]\n",
      " [-0.56222711  0.1568245   0.40562186]\n",
      " [-0.25961337  0.01138562  0.24825302]]\n",
      "16000 번째 학습중입니다.\n",
      "Accuracy :  0.8\n",
      "Loss :      0.573698272062\n",
      "[[ 0.14773018  0.01436338 -0.1620017 ]\n",
      " [ 0.39169448 -0.15019792 -0.24151335]\n",
      " [-0.57817663  0.16037857  0.41801732]\n",
      " [-0.26692371  0.01008689  0.25686208]]\n",
      "17000 번째 학습중입니다.\n",
      "Accuracy :  0.8\n",
      "Loss :      0.566106619537\n",
      "[[ 0.15176808  0.01622976 -0.16790596]\n",
      " [ 0.40233948 -0.15410897 -0.24824731]\n",
      " [-0.59357386  0.16370434  0.43008878]\n",
      " [-0.27397679  0.00869607  0.26530599]]\n",
      "18000 번째 학습중입니다.\n",
      "Accuracy :  0.8066666666666666\n",
      "Loss :      0.558953658638\n",
      "[[ 0.15566259  0.01813583 -0.17370655]\n",
      " [ 0.4126284  -0.15786381 -0.25478139]\n",
      " [-0.60845635  0.16681694  0.44185867]\n",
      " [-0.28079041  0.00722017  0.27359551]]\n",
      "19000 번째 학습중입니다.\n",
      "Accuracy :  0.8133333333333334\n",
      "Loss :      0.552196739775\n",
      "[[ 0.15942509  0.02007735 -0.17941057]\n",
      " [ 0.42258614 -0.1614725  -0.26113043]\n",
      " [-0.62285822  0.16973024  0.45334724]\n",
      " [-0.28738073  0.00566562  0.28174038]]\n",
      "20000 번째 학습중입니다.\n",
      "Accuracy :  0.8200000000000001\n",
      "Loss :      0.545798517545\n",
      "[[ 0.16306575  0.02205051 -0.18502439]\n",
      " [ 0.43223522 -0.16494423 -0.26730779]\n",
      " [-0.63681054  0.17245701  0.46457278]\n",
      " [-0.29376243  0.00403831  0.28974938]]\n",
      "21000 번째 학습중입니다.\n",
      "Accuracy :  0.84\n",
      "Loss :      0.539726173912\n",
      "[[ 0.16659369  0.0240519  -0.19055372]\n",
      " [ 0.4415961  -0.16828738 -0.27332551]\n",
      " [-0.65034163  0.17500901  0.47555188]\n",
      " [-0.29994887  0.00234366  0.29763048]]\n",
      "22000 번째 학습중입니다.\n",
      "Accuracy :  0.8533333333333333\n",
      "Loss :      0.533950769744\n",
      "[[  1.70017121e-01   2.60784484e-02  -1.96003704e-01]\n",
      " [  4.50687351e-01  -1.71509655e-01  -2.79194490e-01]\n",
      " [ -6.63477404e-01   1.77397048e-01   4.86299617e-01]\n",
      " [ -3.05952268e-01   5.86618285e-04   3.05390918e-01]]\n",
      "23000 번째 학습중입니다.\n",
      "Accuracy :  0.8533333333333333\n",
      "Loss :      0.528446701472\n",
      "[[ 0.17334345  0.02812739 -0.20137898]\n",
      " [ 0.45952593 -0.1746181  -0.28492462]\n",
      " [-0.67624157  0.17963112  0.49682971]\n",
      " [-0.31178378 -0.00122823  0.31303728]]\n",
      "24000 번째 학습중입니다.\n",
      "Accuracy :  0.86\n",
      "Loss :      0.523191244169\n",
      "[[ 0.17657939  0.03019623 -0.20668376]\n",
      " [ 0.46812731 -0.17761921 -0.29052489]\n",
      " [-0.68865589  0.18172044  0.50715472]\n",
      " [-0.31745364 -0.0030967   0.32057561]]\n",
      "25000 번째 학습중입니다.\n",
      "Accuracy :  0.8666666666666667\n",
      "Loss :      0.518164165879\n",
      "[[ 0.179731    0.0322827  -0.21192184]\n",
      " [ 0.47650566 -0.18051897 -0.29600349]\n",
      " [-0.70074038  0.18367354  0.5172861 ]\n",
      " [-0.32297124 -0.00501489  0.3280114 ]]\n",
      "26000 번째 학습중입니다.\n",
      "Accuracy :  0.8733333333333333\n",
      "Loss :      0.513347400917\n",
      "[[ 0.18280383  0.03438474 -0.2170967 ]\n",
      " [ 0.48467398 -0.18332288 -0.3013679 ]\n",
      " [-0.71251346  0.1854983   0.52723441]\n",
      " [-0.32834522 -0.00697924  0.33534973]]\n",
      "27000 번째 학습중입니다.\n",
      "Accuracy :  0.8733333333333333\n",
      "Loss :      0.508724772143\n",
      "[[ 0.18580289  0.03650049 -0.22221151]\n",
      " [ 0.49264421 -0.18603603 -0.30662497]\n",
      " [-0.72399212  0.18720206  0.53700932]\n",
      " [-0.33358355 -0.00898643  0.34259525]]\n",
      "28000 번째 학습중입니다.\n",
      "Accuracy :  0.8733333333333333\n",
      "Loss :      0.504281754044\n",
      "[[ 0.18873279  0.03862823 -0.22726915]\n",
      " [ 0.50042735 -0.18866316 -0.31178099]\n",
      " [-0.73519207  0.18879159  0.54661975]\n",
      " [-0.33869357 -0.01103341  0.34975224]]\n",
      "29000 번째 학습중입니다.\n",
      "Accuracy :  0.88\n",
      "Loss :      0.500005269953\n",
      "[[ 0.19159772  0.04076642 -0.23227228]\n",
      " [ 0.50803355 -0.19120861 -0.31684173]\n",
      " [-0.74612786  0.1902732   0.55607392]\n",
      " [-0.34368208 -0.01311733  0.35682468]]\n",
      "30000 번째 학습중입니다.\n",
      "Accuracy :  0.88\n",
      "Loss :      0.495883517911\n",
      "[[ 0.19440155  0.04291364 -0.23722332]\n",
      " [ 0.51547218 -0.19367646 -0.32181251]\n",
      " [-0.75681297  0.19165277  0.56537946]\n",
      " [-0.34855538 -0.01523558  0.36381622]]\n",
      "31000 번째 학습중입니다.\n",
      "Accuracy :  0.88\n",
      "Loss :      0.491905820648\n",
      "[[ 0.1971478   0.04506858 -0.24212452]\n",
      " [ 0.52275191 -0.19607048 -0.32669822]\n",
      " [-0.76725992  0.19293577  0.57454342]\n",
      " [-0.35331931 -0.01738569  0.37073027]]\n",
      "32000 번째 학습중입니다.\n",
      "Accuracy :  0.8866666666666667\n",
      "Loss :      0.488062495946\n",
      "[[ 0.19983974  0.04723006 -0.24697794]\n",
      " [ 0.52988079 -0.19839418 -0.33150341]\n",
      " [-0.77748035  0.19412729  0.58357232]\n",
      " [-0.3579793  -0.01956541  0.37756998]]\n",
      "33000 번째 학습중입니다.\n",
      "Accuracy :  0.8933333333333333\n",
      "Loss :      0.484344744271\n",
      "[[ 0.20248037  0.04939698 -0.25178548]\n",
      " [ 0.5368663  -0.20065082 -0.33623227]\n",
      " [-0.78748512  0.19523211  0.59247227]\n",
      " [-0.3625404  -0.02177262  0.38433829]]\n",
      "34000 번째 학습중입니다.\n",
      "Accuracy :  0.9\n",
      "Loss :      0.480744551094\n",
      "[[ 0.20507244  0.05156833 -0.25654891]\n",
      " [ 0.54371537 -0.20284348 -0.34088868]\n",
      " [-0.79728432  0.19625467  0.60124891]\n",
      " [-0.36700731 -0.02400534  0.39103792]]\n",
      "35000 번째 학습중입니다.\n",
      "Accuracy :  0.9066666666666666\n",
      "Loss :      0.477254601747\n",
      "[[ 0.20761853  0.0537432  -0.26126986]\n",
      " [ 0.55043448 -0.20497501 -0.34547626]\n",
      " [-0.80688741  0.19719914  0.60990754]\n",
      " [-0.37138442 -0.02626175  0.39767144]]\n",
      "36000 번째 학습중입니다.\n",
      "Accuracy :  0.9066666666666666\n",
      "Loss :      0.473868206984\n",
      "[[ 0.21012099  0.05592074 -0.26594986]\n",
      " [ 0.55702966 -0.20704809 -0.34999836]\n",
      " [-0.81630322  0.19806942  0.61845306]\n",
      " [-0.37567585 -0.02854012  0.40424124]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37000 번째 학습중입니다.\n",
      "Accuracy :  0.9066666666666666\n",
      "Loss :      0.470579237753\n",
      "[[ 0.21258203  0.05810015 -0.27059031]\n",
      " [ 0.56350654 -0.20906522 -0.35445811]\n",
      " [-0.82554003  0.19886919  0.6268901 ]\n",
      " [-0.37988541 -0.03083886  0.41074954]]\n",
      "38000 번째 학습중입니다.\n",
      "Accuracy :  0.9066666666666666\n",
      "Loss :      0.467382067874\n",
      "[[ 0.21500368  0.06028072 -0.27519254]\n",
      " [ 0.56987039 -0.21102876 -0.35885842]\n",
      " [-0.83460558  0.19960187  0.63522297]\n",
      " [-0.38401672 -0.03315646  0.41719845]]\n",
      "39000 번째 학습중입니다.\n",
      "Accuracy :  0.92\n",
      "Loss :      0.464271523547\n",
      "[[ 0.21738785  0.06246177 -0.27975776]\n",
      " [ 0.57612614 -0.21294092 -0.36320201]\n",
      " [-0.84350715  0.2002707   0.64345571]\n",
      " [-0.38807314 -0.03549151  0.42358992]]\n",
      "40000 번째 학습중입니다.\n",
      "Accuracy :  0.92\n",
      "Loss :      0.461242838773\n",
      "[[ 0.21973629  0.0646427  -0.28428713]\n",
      " [ 0.58227841 -0.21480378 -0.36749142]\n",
      " [-0.85225159  0.20087872  0.65159212]\n",
      " [-0.39205784 -0.03784271  0.42992582]]\n",
      "41000 번째 학습중입니다.\n",
      "Accuracy :  0.92\n",
      "Loss :      0.458291615896\n",
      "[[ 0.22205065  0.06682293 -0.28878172]\n",
      " [ 0.58833154 -0.2166193  -0.37172904]\n",
      " [-0.86084531  0.2014288   0.65963577]\n",
      " [-0.39597378 -0.04020882  0.43620787]]\n",
      "42000 번째 학습중입니다.\n",
      "Accuracy :  0.92\n",
      "Loss :      0.455413790605\n",
      "[[ 0.22433247  0.06900193 -0.29324254]\n",
      " [ 0.5942896  -0.21838931 -0.37591708]\n",
      " [-0.86929438  0.20192363  0.66759002]\n",
      " [-0.39982378 -0.04258867  0.44243773]]\n",
      "43000 번째 학습중입니다.\n",
      "Accuracy :  0.92\n",
      "Loss :      0.452605600822\n",
      "[[ 0.22658318  0.07117922 -0.29767053]\n",
      " [ 0.60015641 -0.22011557 -0.38005764]\n",
      " [-0.87760451  0.20236575  0.67545802]\n",
      " [-0.40361047 -0.04498119  0.44861693]]\n",
      "44000 번째 학습중입니다.\n",
      "Accuracy :  0.92\n",
      "Loss :      0.449863558987\n",
      "[[ 0.22880412  0.07335433 -0.30206659]\n",
      " [ 0.6059356  -0.22179972 -0.38415268]\n",
      " [-0.88578108  0.20275757  0.68324277]\n",
      " [-0.40733634 -0.04738534  0.45474695]]\n",
      "45000 번째 학습중입니다.\n",
      "Accuracy :  0.9266666666666666\n",
      "Loss :      0.447184427321\n",
      "[[ 0.23099655  0.07552686 -0.30643154]\n",
      " [ 0.61163055 -0.22344331 -0.38820404]\n",
      " [-0.89382918  0.20310137  0.69094707]\n",
      " [-0.41100372 -0.04980017  0.46082916]]\n",
      "46000 번째 학습중입니다.\n",
      "Accuracy :  0.9266666666666666\n",
      "Loss :      0.444565195694\n",
      "[[ 0.23316164  0.07769641 -0.31076618]\n",
      " [ 0.61724447 -0.22504781 -0.39221345]\n",
      " [-0.90175362  0.20339929  0.69857359]\n",
      " [-0.41461484 -0.05222476  0.46686488]]\n",
      "47000 번째 학습중입니다.\n",
      "Accuracy :  0.9333333333333333\n",
      "Loss :      0.442003061798\n",
      "[[ 0.2353005   0.07986262 -0.31507126]\n",
      " [ 0.62278039 -0.22661462 -0.39618256]\n",
      " [-0.90955895  0.20365336  0.70612484]\n",
      " [-0.4181718  -0.05465827  0.47285534]]\n",
      "48000 번째 학습중입니다.\n",
      "Accuracy :  0.9333333333333333\n",
      "Loss :      0.439495413347\n",
      "[[ 0.23741417  0.08202516 -0.31934746]\n",
      " [ 0.62824117 -0.22814507 -0.40011289]\n",
      " [-0.91724947  0.20386552  0.71360321]\n",
      " [-0.42167658 -0.05709988  0.47880173]]\n",
      "49000 번째 학습중입니다.\n",
      "Accuracy :  0.94\n",
      "Loss :      0.43703981206\n",
      "[[ 0.23950361  0.08418372 -0.32359547]\n",
      " [ 0.6336295  -0.22964039 -0.40400591]\n",
      " [-0.92482928  0.20403758  0.72101095]\n",
      " [-0.42513105 -0.05954885  0.48470516]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(50000):\n",
    "    softmax.gradient(x, y)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(i, \"번째 학습중입니다.\")\n",
    "        print(\"Accuracy : \", softmax.accuracy(x, y))\n",
    "        print(\"Loss :     \", softmax.loss(x, y))\n",
    "        print(softmax.params['W'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
