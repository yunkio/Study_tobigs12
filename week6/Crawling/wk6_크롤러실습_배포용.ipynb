{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import sleep\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from random import randint\n",
    "\n",
    "# Import BeautifulSoup and requests module\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; InteSl Mac O X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "my_url = \"http://www.pythonscraping.com/exercises/exercise1.html\"\n",
    "\n",
    "# 사이트 요청 \n",
    "response = requests.get(my_url, headers=headers)\n",
    "\n",
    "# 요청의 status code 보기 \n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HTML 보기\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find & FindAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title tag 찾기 \n",
    "soup = bs(response.text)\n",
    "soup.find('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title tag 내용 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSS Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://www.pythonscraping.com/pages/warandpeace.html\"\n",
    "response = requests.get(url, headers=headers)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anna\\nPavlovna Scherer'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초록색 단어 찾기\n",
    "soup = bs(response.text, 'html.parser')\n",
    "soup.find('span', {\"class\" : \"green\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-36-bfbbbb60fc37>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-36-bfbbbb60fc37>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    green_word_tags = soup.findAll('span'.class_=\"green\")\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": [
    "# 모든 초록색 단어 찾기 (findAll)\n",
    "soup = bs(response.text, 'html.parser')\n",
    "soup.findAll('span', {\"class\" : \"green\"})\n",
    "\n",
    "green_word_tags = soup.findAll('span',class_=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빨간색 단어를 추출해봅시다 \n",
    "red_word_tags = soup.findAll('span',class_=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'text'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-f9f377f61615>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# <span> 태그에 존재하는 빨간색과 초록색 글씨를 모두 추출해봅시다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"red\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"class\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"green\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1618\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1619\u001b[0m         raise AttributeError(\n\u001b[1;32m-> 1620\u001b[1;33m             \u001b[1;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1621\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'text'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "# <span> 태그에 존재하는 빨간색과 초록색 글씨를 모두 추출해봅시다\n",
    "soup.findAll('span', {\"class\" : \"red\", \"class\" : \"green\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복습: <p> 에 있는 내용을 모두 리스트에 저장해봅시다. \n",
    "paragraph_tags = soup.findAll('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실제 사이트에 연습:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카페\n",
      "메일\n",
      "뉴스\n",
      "지도\n",
      "증권\n",
      "쇼핑\n",
      "카카오TV\n",
      "웹툰\n",
      "블로그\n",
      "브런치\n",
      "사전\n",
      "게임\n",
      "같이가치\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.daum.net'\n",
    "\n",
    "#Request 요청 (headers 추가 필수)\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = bs(response.text)\n",
    "\n",
    "# 탭버튼이 담긴 정보 찾기 \n",
    "big_tab = soup.find('ul',{\"class\":\"list_mainsvc\"})\n",
    "\n",
    "#findAll 도 해보기\n",
    "for i in big_tab.findAll('a'):\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 태그 안에 있는 URL 정보를 얻고 싶다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://cafe.daum.net/'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find 문 이용시 \n",
    "tabs = big_tab.findAll('a')\n",
    "cafe = tabs[0]\n",
    "cafe['href']\n",
    "# FindALL 문 이용시 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parent & Sibling 관계 이용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://pythonscraping.com/pages/page3.html\"\n",
    "response = requests.get(url)\n",
    "soup = bs(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 이미지 찾기 \n",
    "first_image_table = soup.find('img',{'src': \"../img/gifts/img1.jpg\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n$15.00\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parent& sibling 관계를 이용하여\n",
    "# 다음 행에 있는 이미지 URL 구하기! \n",
    "first_image_table.parent.previous_sibling.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../img/gifts/logo.jpg\n",
      "../img/gifts/img1.jpg\n",
      "../img/gifts/img2.jpg\n",
      "../img/gifts/img3.jpg\n",
      "../img/gifts/img4.jpg\n",
      "../img/gifts/img6.jpg\n"
     ]
    }
   ],
   "source": [
    "# 복습: findAll 문을 이용하여 모든 이미지 URl 구하기\n",
    "images_tags = soup.findAll('img')\n",
    "for i in images_tags:\n",
    "    print(i['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복습: CSS 문을 이용하여 볼드체 글씨를 모두 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression 이용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img src=\"../img/gifts/img1.jpg\"/>,\n",
       " <img src=\"../img/gifts/img2.jpg\"/>,\n",
       " <img src=\"../img/gifts/img3.jpg\"/>,\n",
       " <img src=\"../img/gifts/img4.jpg\"/>,\n",
       " <img src=\"../img/gifts/img6.jpg\"/>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# 정규식을 활용하여 gift 이미지 URL만 구하기 \n",
    "# 정규식은 제공 \n",
    "gift_regex = re.compile('img\\d.jpg')\n",
    "soup.findAll('img',{\"src\":gift_regex})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실제 크롤링 연습:  다음 영화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['명량'],\n",
       " ['겨울왕국'],\n",
       " ['인터스텔라'],\n",
       " ['해적: 바다로 간 산적'],\n",
       " ['수상한 그녀'],\n",
       " ['변호인'],\n",
       " ['국제시장'],\n",
       " ['트랜스포머: 사라진 시대'],\n",
       " ['군도: 민란의 시대'],\n",
       " ['엣지 오브 투모로우'],\n",
       " ['엑스맨: 데이즈 오브 퓨처 패스트'],\n",
       " ['어메이징 스파이더맨 2'],\n",
       " ['타짜-신의 손'],\n",
       " ['혹성탈출: 반격의 서막'],\n",
       " ['캡틴 아메리카: 윈터 솔져'],\n",
       " ['역린'],\n",
       " ['님아, 그 강을 건너지 마오'],\n",
       " ['신의 한 수'],\n",
       " ['끝까지 간다'],\n",
       " ['비긴 어게인'],\n",
       " ['드래곤 길들이기 2'],\n",
       " ['표적'],\n",
       " ['메이즈 러너'],\n",
       " ['호빗: 다섯 군대 전투'],\n",
       " ['나의 사랑 나의 신부'],\n",
       " ['논스톱'],\n",
       " ['인투 더 스톰'],\n",
       " ['노아'],\n",
       " ['남자가 사랑할 때'],\n",
       " ['루시'],\n",
       " ['용의자'],\n",
       " ['기술자들'],\n",
       " ['나를 찾아줘'],\n",
       " ['제보자'],\n",
       " ['피끓는 청춘'],\n",
       " ['두근두근 내 인생'],\n",
       " ['우아한 거짓말'],\n",
       " ['300: 제국의 부활'],\n",
       " ['엑소더스: 신들과 왕들'],\n",
       " ['해무(海霧)'],\n",
       " ['인간중독'],\n",
       " ['말레피센트'],\n",
       " ['폼페이: 최후의 날'],\n",
       " ['퓨리'],\n",
       " ['드라큘라: 전설의 시작'],\n",
       " ['가디언즈 오브 갤럭시'],\n",
       " ['찌라시: 위험한 소문'],\n",
       " ['빅매치'],\n",
       " ['슬로우 비디오'],\n",
       " ['방황하는 칼날']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def daum_movie_crawler(year):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; InteSl Mac O X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    main_url = 'https://movie.daum.net/boxoffice/yearly?year=' + str(year)\n",
    "    \n",
    "    # Initialize \n",
    "    data = []\n",
    "    cnt= 0 \n",
    "    \n",
    "    response = requests.get(main_url,headers=headers)\n",
    "    body_table = bs(response.text,'html.parser')\n",
    "    \n",
    "    #포스터 테이블 구하기 \n",
    "    poster_table = body_table.findAll('div',{\"class\" : 'wrap_movie'})\n",
    "\n",
    "    \n",
    "    #포스터 테이블을 looping \n",
    "    # 제목과 평론가 평점 찾기\n",
    "    for poster in poster_table:\n",
    "        title_tag = poster.find('a',{'class' : \"name_movie #title\"})\n",
    "        data.append([title_tag.text])                                 \n",
    "                                                      \n",
    "    return data\n",
    "\n",
    "daum_movie_crawler(2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2009-2019년도 데이터 얻기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예외/ 오류 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    response = requests.get(my_url, headers=headers)\n",
    "except Exception as e:\n",
    "    print('Error occured while requesting')\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "else:\n",
    "    print('Status code Error ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "위에 daum_movie_crawler 에 try-except 문을 추가해서\n",
    "오류가 나는 것을 방지해보기 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "from selenium import webdriver\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "browser = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_url = \"https://www.ncbi.nlm.nih.gov/pubmed/?term=wine\"\n",
    "browser = webdriver.Chrome() #혹은 .Firefox() , .Safari()\n",
    "browser.get(your_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음으로 가기 버튼 xpath 를 이용해 찾기 \n",
    "next_button = browser.find_element_by_xpath('//*[@id=\"EntrezSystem2.PEntrez.PubMed.Pubmed_ResultsPanel.Pubmed_Pager.Page\"]')\n",
    "\n",
    "# 버튼 클릭 \n",
    "next_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#논문 태그 찾기 (class 이용)\n",
    "paper_tags = browser.find_elements_by_class_name('rprt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Combination of Hot Air and Chitosan Treatments on Phytochemical Changes during Postharvest Storage of 'Sanhua' Plum Fruits.\n",
      "Influence of fungicide residues and in vitro gastrointestinal digestion on total antioxidant capacity and phenolic fraction of Graciano and Tempranillo red wines.\n",
      "Comparative transcriptional analysis of flavour-biosynthetic genes of a native Saccharomyces cerevisiae strain fermenting in its natural must environment, vs. a commercial strain and correlation of the genes' activities with the produced flavour compounds.\n",
      "Targeted Drug Delivery in Plants: Enzyme-Responsive Lignin Nanocarriers for the Curative Treatment of the Worldwide Grapevine Trunk Disease Esca.\n",
      "Potential of Cooperage Byproducts Rich in Ellagitannins to Improve the Antioxidant Activity and Color Expression of Red Wine Anthocyanins.\n",
      "Pectolytic enzyme reduces the concentration of colloidal particles in wine due to changes in polysaccharide structure and aggregation properties.\n",
      "Indigenous Yeast Interactions in Dual-Starter Fermentations May Improve the Varietal Expression of Moschofilero Wine.\n",
      "Importance and role of lipids in wine yeast fermentation.\n",
      "Modelling the seasonal changes in the gas exchange response to CO2 in relation to short-term leaf temperature changes in Vitis vinifera cv. Shiraz grapevines grown in outdoor conditions.\n",
      "Wine polysaccharides modulating astringency through the interference on interaction of flavan-3-ols and BSA in model wine.\n",
      "Association between alcohol consumption and survival in colorectal cancer: A meta-analysis.\n",
      "Identification of environmental factors controlling wine quality: A case study in Saint-Emilion Grand Cru appellation, France.\n",
      "Toxicity, attraction, and repellency of toxic baits to stingless bees Plebeia emerina (Friese) and Tetragonisca fiebrigi (Schwarz) (Hymenoptera: Apidae: Meliponini).\n",
      "A crystal structure of a collaborative RNA regulatory complex reveals mechanisms to refine target specificity.\n",
      "Comparison of iodine status pre- and post-mandatory iodine fortification of bread in South Australia: a population study using newborn thyroid-stimulating hormone concentration as a marker.\n",
      "Wine aging: a bottleneck story.\n",
      "Penile prosthesis implant for primary erectile dysfunction in patient with Klippel-Trenaunay syndrome complicated by consumptive coagulopathy: A case report.\n",
      "[Determination of Nonvolatile Amines in Foods by HPLC Following Fluorescamine Derivatization].\n",
      "Clinical characteristics and treatment of 52 cases of phakomatosis pigmentovascularis.\n",
      "Effect of mixed species alcoholic fermentation on growth and malolactic activity of lactic acid bacteria.\n"
     ]
    }
   ],
   "source": [
    "# 각 태그의 제목 (tag_name 이용)\n",
    "for paper in paper_tags : \n",
    "    print(paper.find_element_by_tag_name('a').text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5 페이지동안 논문제목을 크롤링한뒤 \n",
    "논문제목이 담긴 리스트를 반환하는 함수를 구현해보세요 (다음 페이지로 넘어가기전에 0-1초간 휴식)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome() \n",
    "browser.get(your_url)\n",
    "\n",
    "for page in range(1,6):\n",
    "    print(\"Currently on page \", page)\n",
    "    paper_tags = browser.find_elements_by_class_name('rprt')\n",
    "    for paper in paper_tags : \n",
    "        #paper 찾기\n",
    "        #data.append(paper 제목)\n",
    "    #next_button 찾기\n",
    "    #next_buttn 클릭하기\n",
    "    sleep(random.uniform(0,1))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
