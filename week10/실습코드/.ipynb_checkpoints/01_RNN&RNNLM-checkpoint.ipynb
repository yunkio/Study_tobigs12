{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 시계열 데이터를 다루기 위한 신경망입니다.  \n",
    "Recurrent Neural Network라는 용어명에서도 알 수 있듯이 \"순환\"하는 경로가 존재하여 데이터가 순환됩니다.  \n",
    "즉, 과거의 정보를 \"기억\"하는 동시에 이에 새로운 정보를 추가해 \"갱신\"하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Layer의 Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$\\mathbf{h_t} = \\text{tanh}(\\mathbf{h_{t-1}W_h + x_tW_x+b})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN Layer 또한 데이터를 Mini-Batch로 처리합니다. 따라서 $\\mathbf{x_t}$와 $\\mathbf{h_t}$에 각 샘플 데이터를 행 방향으로 묶어 처리합니다.  \n",
    "따라서 아래와 같은 shape을 가지게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\begin{array}{lcl}\n",
    "\\mathbf{h_{t-1}} & = & N \\times H \\\\\n",
    "\\mathbf{W_h} & = & H \\times H \\\\\n",
    "\\mathbf{x_{t}} & = & N \\times D \\\\\n",
    "\\mathbf{W_x} & = & D \\times H \\\\\n",
    "\\mathbf{b_t} & = & N \\times H \\\\\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        '''\n",
    "        NN의 weight를 초기화하고, backward를 위한 grad와 forward값을 인스턴스 변수로 저장해두는 부분입니다.\n",
    "        '''\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        '''\n",
    "        input 값으로 아래 두 가지를 받습니다.\n",
    "        x : 현재 시점 t에서의 입력\n",
    "        h_prev : 이전 시점 t-1에서의 hidden state\n",
    "        '''\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t) # 현시점의 출력(다음 시각 t+1로의 입력)\n",
    "\n",
    "        self.cache = (x, h_prev, h_next) # backward를 위해 forward시의 값을 저장해둡니다.\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        '''\n",
    "        dh_next : 다음 시각 t+1로부터 내려온 gradient\n",
    "        아래의 backward graph를 참고\n",
    "        '''\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "\n",
    "        dt = dh_next * (1 - h_next ** 2) # (N x H)\n",
    "        db = np.sum(dt, axis=0)    # (N x H) ==> (1 x H)\n",
    "        dWh = np.dot(h_prev.T, dt) # (H x N) @ (N x H) ==> (H x H)\n",
    "        dh_prev = np.dot(dt, Wh.T) # (N x H) @ (H x H) ==> (N x H)\n",
    "        dWx = np.dot(x.T, dt)      # (D x N) @ (N x H) ==> (D x H)\n",
    "        dx = np.dot(dt, Wx.T)      # (N x H) @ (H x D) ==> (N x D)\n",
    "\n",
    "        self.grads[0][...] = dWx # cf. [...] : deep copy : \n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        return dx, dh_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rnn_backprop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time RNN Layer 구현\n",
    "- T개의 RNN 계층으로 구성된 RNN Layer을 만듭니다.  \n",
    "- 즉, 시계열 데이터 길이가 T인 Truncated BPTT를 하기위한 Layer입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rnn_timernn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation은 다음과 같이 이뤄집니다.  \n",
    "output layer에서 내려오는 기울기를 dhs, 아래로 내보내는 기울기를 dxs라 표기합니다.  \n",
    "Truncated BPTT이므로 dh는 필요하지 않습니다. 그러나 뒤에 다룰 seq2seq 모델에서 사용하므로 만들어둡니다.  \n",
    "h는 여기선 영벡터입니다. 최초 RNN layer에 대한 이전 상태의 hidden state로 넣어주는 값입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rnn_timernn_back.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t번쨰 RNN에서의 backpropgation시 $\\mathbf{dh_t}$와 $\\mathbf{dh_{next}}$가 합산되어 전해집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rnn_timernn_backprop_at_t.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        '''\n",
    "        NN의 weight를 초기화하고\n",
    "        forward를 위한 hidden state h와\n",
    "        backward를 위한 grad와 forward, 그리고 dh값을 인스턴스 변수로 저장해두는 부분입니다.\n",
    "        '''\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        # forward() 메서드 호출시 h는 마지막 RNN hidden state를 저장하고\n",
    "        # backward() 메서드 호출시 dh는 한 시점 앞 블록의 hiddenstate의 gradient를 저장합니다(위에서 내려오는 기울기)\n",
    "        self.h, self.dh = None, None\n",
    "        \n",
    "        # 한 번의 Backward연산이 끝나고 새로운 데이터를 받을 때 이전의 hidden state를 유지할 것인지를 정합니다.\n",
    "        # 전체 데이터가 하나로 쭉 이어진 시계열 데이터라면 유지해야 합니다.\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        '''\n",
    "        input 값으로 Batch size = N, 시간축 길이 = T, embedding dimension = D 인 데이터를 받습니다.\n",
    "        '''\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "\n",
    "        # 각 시점에서의 RNN layer를 self.layers 리스트에 저장합니다.\n",
    "        # hs : 각 시점에서의 hidden state를 저장해두기 tensor입니다.\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "        \n",
    "        '''\n",
    "        self.stateful이 False 즉, hidden state를 유지하지 않기로 설정했거나,\n",
    "        self.h가 False, 즉 최초 실행시에는 self.h를 영벡터로 초기화합니다.\n",
    "        RNN이 이전 시점의 hidden state를 입력으로 받는데, 최초 실행시는 이전 hidden state가 없기 때문에\n",
    "        영벡터를 입력으로 넣어줍니다.\n",
    "        '''\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        # 시간축 길이 T만큼 for문을 돌면서 \n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params) # RNN layer를 생성하고\n",
    "            self.h = layer.forward(xs[:, t, :], self.h) # 시점 t의 데이터 x_t와 h_(t-1)을 입력해 forward한 뒤\n",
    "            hs[:, t, :] = self.h   # hidden state를 t시점의 위치에 저장하고\n",
    "            self.layers.append(layer) # 해당 layer를 리스트에 저장합니다(backward시 필요)\n",
    "            \n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        '''\n",
    "        dhs : output layer 쪽에서 전해지는 기울기\n",
    "        '''\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        '''\n",
    "        dxs : 아래로 내보내는 gradient를 담을 빈 Tensor\n",
    "        dh : seq2seq 모델을 위해서 저장해두는 hidden state\n",
    "        grads : 각각 Wx, Wh, b에 대한 gradient가 저장된다.\n",
    "                이 때, 각 시점의 RNN layer들은 모두 동일한 weights를 공유하므로\n",
    "                 backward시에 각 시점의 gradient가 모두 합산된다.\n",
    "        '''\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0   # dh_next(이전 시점의 hidden state의 gradient)를 의미\n",
    "                 # backward시 가장 마지막 RNN layer의 dh_next는 없으므로 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)): # forward와는 달리 거꾸로 for문을 수행\n",
    "            layer = self.layers[t]   # forward시 저장한 layer를 가져와서\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh) # outputlayer의 t시점에서 내려온 gradient인 dh_t와 dh_next를 더하여 backward\n",
    "            dxs[:, t, :] = dx  # 아래로 내보낼 t시점의 gradient dxs_t를 저장함\n",
    "\n",
    "            for i, grad in enumerate(layer.grads): # 현재 backward하는 layer의 Wx, Wh b에 대한 gradients를 누적합 \n",
    "                grads[i] += grad\n",
    "                \n",
    "        # backward가 모두 끝나면 Wx, Wh, b에 대한 gradients를 인스턴스 변수에 저장\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh # 최종 hidden state의 gradients를 저장해둠(for seq2seq)\n",
    "\n",
    "        return dxs\n",
    "\n",
    "    # hidden state를 임의의 값으로 설정하거나 초기화하기 위한 메서드\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RNN Language Model(RNNLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN을 사용해서 Languge Model을 구현합니다.  \n",
    "먼저 RNNLM의 신경망 구조는 아래와 같습니다.  \n",
    "왼쪽은 Layer 구성이고, 오른쪽은 시간축으로 펼친 그림입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RNNLM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W_t 는 t시점에서의 단어(또는 token)입니다.  \n",
    "가장 아래에 **Embedding layer**를 볼 수 있는데, NLP Basic에서 배웠던 word2vec 모델이 전체 모델에 포함된 것이라 할 수 있습니다. 입력 단어의 one hot encoding과 weight($\\mathbf{W_E}$)를 dot product하여\n",
    "나온 값을 해당 단어의 embedding값으로 사용합니다.  \n",
    "예를 들면 단어 \"dog\"이 전체 단어목록 V에서 3번째 단어이고 전체 단어(V)가 100개이면,  \n",
    "embedding layer의 weight($\\mathbf{W_E}$)는 (100 x D)이고, (D는 embedding dimension)  \n",
    "\"dog\"의 W_t는 [0, 0, 1, ..., 0, 0]으로 표현됩니다. 이를 embedding weight와 dot product한 결과는 embedding weight의 3번째 행을 뽑아낸 결과가 됩니다. 즉 해당 단어의 word embedding 값입니다.  \n",
    "embedding weight는 이전에 word embedding한 값을 불러와 사용할 수 있으며, 또는 지금처럼 random으로 초기화한 후 모델의 학습이 진행됨에 따라 적절한 word embedding값을 얻을 수도 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 **RNN layer**를 지나, **Affine layer**를 거쳐 **Softmax layer**를 통과합니다.\n",
    "**Softmax layer**를 통과한 결과는 각 시점(t)에서 예측한 다음 시점(t+1)에 올 단어의 확률 분포입니다.  \n",
    "여기서 **Affine layer**는 우리가 MLP에서 봤었던 Activation function에 통과하기 이전의 선형변환을 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 모델이 학습을 하려면 Loss function이 있어야 합니다. 위의 그림에 Loss function을 추가하면 아래 그림과 같습니다.  \n",
    "각 시점에서의 Loss를 모두 계산하여 이를 평균한 값을 Loss로 취합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RNNLM_loss.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNLM CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.time_layers import *\n",
    "\n",
    "\n",
    "class SimpleRnnlm:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn #메서드를 짧게 표현\n",
    "\n",
    "        # 가중치 초기화(Xavior)\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        rnn_b = np.zeros(H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W), # 시간길이 T인 입력데이터에 대해 Embedding layer를 한 번에 처리하는 layer\n",
    "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True), # 시간길이 T인 입력데이터에 대한 RNN(Truncated RNN)\n",
    "            TimeAffine(affine_W, affine_b) # 시간길이 T인 입력데이터에 대해 Affine 변환을 한 번에 처리하는 Layer\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss() # 시간길이 T인 입력데이터에 대해 Softmax cross entropy loss를 한 번에 계산하는 layer\n",
    "        self.rnn_layer = self.layers[1]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNLM Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langube Model의 평가\n",
    "- **perplexity**\n",
    "- 쉽게 말해 확률의 역수로, 다음에 등장할 수 있는 단어의 분기수. 즉 다음에 가능한 단어의 후보 수\n",
    "- 좋은 모델일 수록 다음에 등장할 단어를 높은 확률로 예측한다.  \n",
    "예를 들어 \"You say goodbye and I say hello\"라는 copurs로 학습했을 때,\n",
    "  \"You\" 다음에 올 단어로 \"say\"를 0.8의 확률로 예측했다면 perplexity는 1/0.8 = 1.25 가 되며 0.5의 확률로 예측했다면 perplexity = 1/0.5 = 2.5가 된다.  \n",
    "- perplexity가 낮을 수록 좋은 모델\n",
    "- 다음에 가능한 후보의 수를 많이 좁힐 수록 모델의 성능이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$ L = - \\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}\\text{log}y_{nk} $$\n",
    "# $$ \\mathbf{preplexity} = e^L $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L$은 softmax with crossentropy loss를 의미함. 다음 단어를 하나의 단어로 특정지을수록 Loss가 줄어들고, perplexity 또한 줄어든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말뭉치 크기: 1000, 어휘 수: 418\n",
      "| 에폭 1 | 퍼플렉서티 379.33\n",
      "| 에폭 2 | 퍼플렉서티 256.43\n",
      "| 에폭 3 | 퍼플렉서티 221.32\n",
      "| 에폭 4 | 퍼플렉서티 212.03\n",
      "| 에폭 5 | 퍼플렉서티 204.15\n",
      "| 에폭 6 | 퍼플렉서티 201.58\n",
      "| 에폭 7 | 퍼플렉서티 197.23\n",
      "| 에폭 8 | 퍼플렉서티 195.03\n",
      "| 에폭 9 | 퍼플렉서티 190.10\n",
      "| 에폭 10 | 퍼플렉서티 191.48\n",
      "| 에폭 11 | 퍼플렉서티 188.33\n",
      "| 에폭 12 | 퍼플렉서티 191.53\n",
      "| 에폭 13 | 퍼플렉서티 190.00\n",
      "| 에폭 14 | 퍼플렉서티 189.61\n",
      "| 에폭 15 | 퍼플렉서티 188.98\n",
      "| 에폭 16 | 퍼플렉서티 186.15\n",
      "| 에폭 17 | 퍼플렉서티 182.70\n",
      "| 에폭 18 | 퍼플렉서티 179.92\n",
      "| 에폭 19 | 퍼플렉서티 180.67\n",
      "| 에폭 20 | 퍼플렉서티 182.25\n",
      "| 에폭 21 | 퍼플렉서티 180.85\n",
      "| 에폭 22 | 퍼플렉서티 178.06\n",
      "| 에폭 23 | 퍼플렉서티 175.45\n",
      "| 에폭 24 | 퍼플렉서티 174.71\n",
      "| 에폭 25 | 퍼플렉서티 174.26\n",
      "| 에폭 26 | 퍼플렉서티 172.35\n",
      "| 에폭 27 | 퍼플렉서티 168.42\n",
      "| 에폭 28 | 퍼플렉서티 167.49\n",
      "| 에폭 29 | 퍼플렉서티 164.68\n",
      "| 에폭 30 | 퍼플렉서티 160.32\n",
      "| 에폭 31 | 퍼플렉서티 160.23\n",
      "| 에폭 32 | 퍼플렉서티 153.94\n",
      "| 에폭 33 | 퍼플렉서티 155.34\n",
      "| 에폭 34 | 퍼플렉서티 148.11\n",
      "| 에폭 35 | 퍼플렉서티 147.33\n",
      "| 에폭 36 | 퍼플렉서티 140.65\n",
      "| 에폭 37 | 퍼플렉서티 137.97\n",
      "| 에폭 38 | 퍼플렉서티 133.90\n",
      "| 에폭 39 | 퍼플렉서티 128.77\n",
      "| 에폭 40 | 퍼플렉서티 123.52\n",
      "| 에폭 41 | 퍼플렉서티 122.62\n",
      "| 에폭 42 | 퍼플렉서티 117.01\n",
      "| 에폭 43 | 퍼플렉서티 111.97\n",
      "| 에폭 44 | 퍼플렉서티 104.49\n",
      "| 에폭 45 | 퍼플렉서티 103.10\n",
      "| 에폭 46 | 퍼플렉서티 102.50\n",
      "| 에폭 47 | 퍼플렉서티 96.76\n",
      "| 에폭 48 | 퍼플렉서티 91.60\n",
      "| 에폭 49 | 퍼플렉서티 87.84\n",
      "| 에폭 50 | 퍼플렉서티 81.51\n",
      "| 에폭 51 | 퍼플렉서티 78.84\n",
      "| 에폭 52 | 퍼플렉서티 75.59\n",
      "| 에폭 53 | 퍼플렉서티 71.31\n",
      "| 에폭 54 | 퍼플렉서티 69.31\n",
      "| 에폭 55 | 퍼플렉서티 66.10\n",
      "| 에폭 56 | 퍼플렉서티 61.41\n",
      "| 에폭 57 | 퍼플렉서티 59.08\n",
      "| 에폭 58 | 퍼플렉서티 55.53\n",
      "| 에폭 59 | 퍼플렉서티 52.44\n",
      "| 에폭 60 | 퍼플렉서티 48.89\n",
      "| 에폭 61 | 퍼플렉서티 46.89\n",
      "| 에폭 62 | 퍼플렉서티 44.16\n",
      "| 에폭 63 | 퍼플렉서티 40.43\n",
      "| 에폭 64 | 퍼플렉서티 39.14\n",
      "| 에폭 65 | 퍼플렉서티 36.60\n",
      "| 에폭 66 | 퍼플렉서티 34.12\n",
      "| 에폭 67 | 퍼플렉서티 33.27\n",
      "| 에폭 68 | 퍼플렉서티 30.56\n",
      "| 에폭 69 | 퍼플렉서티 29.10\n",
      "| 에폭 70 | 퍼플렉서티 27.75\n",
      "| 에폭 71 | 퍼플렉서티 26.01\n",
      "| 에폭 72 | 퍼플렉서티 24.96\n",
      "| 에폭 73 | 퍼플렉서티 23.30\n",
      "| 에폭 74 | 퍼플렉서티 22.08\n",
      "| 에폭 75 | 퍼플렉서티 21.05\n",
      "| 에폭 76 | 퍼플렉서티 19.30\n",
      "| 에폭 77 | 퍼플렉서티 18.62\n",
      "| 에폭 78 | 퍼플렉서티 17.60\n",
      "| 에폭 79 | 퍼플렉서티 16.48\n",
      "| 에폭 80 | 퍼플렉서티 15.18\n",
      "| 에폭 81 | 퍼플렉서티 15.31\n",
      "| 에폭 82 | 퍼플렉서티 14.54\n",
      "| 에폭 83 | 퍼플렉서티 13.39\n",
      "| 에폭 84 | 퍼플렉서티 12.88\n",
      "| 에폭 85 | 퍼플렉서티 11.81\n",
      "| 에폭 86 | 퍼플렉서티 11.41\n",
      "| 에폭 87 | 퍼플렉서티 11.07\n",
      "| 에폭 88 | 퍼플렉서티 10.38\n",
      "| 에폭 89 | 퍼플렉서티 9.65\n",
      "| 에폭 90 | 퍼플렉서티 9.15\n",
      "| 에폭 91 | 퍼플렉서티 9.01\n",
      "| 에폭 92 | 퍼플렉서티 8.67\n",
      "| 에폭 93 | 퍼플렉서티 8.46\n",
      "| 에폭 94 | 퍼플렉서티 7.33\n",
      "| 에폭 95 | 퍼플렉서티 7.17\n",
      "| 에폭 96 | 퍼플렉서티 7.02\n",
      "| 에폭 97 | 퍼플렉서티 6.65\n",
      "| 에폭 98 | 퍼플렉서티 6.37\n",
      "| 에폭 99 | 퍼플렉서티 5.95\n",
      "| 에폭 100 | 퍼플렉서티 5.76\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.optimizer import SGD\n",
    "from dataset import ptb\n",
    "from simple_rnnlm import SimpleRnnlm\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100 # RNN의 은닉 상태 벡터의 원소 수\n",
    "time_size = 5     # Truncated BPTT가 한 번에 펼치는 시간 크기\n",
    "lr = 0.1\n",
    "max_epoch = 100\n",
    "\n",
    "# 학습 데이터 읽기(전체 중 1000개만)\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_size = 1000\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus) + 1)\n",
    "\n",
    "xs = corpus[:-1]  # 입력\n",
    "ts = corpus[1:]   # 출력(정답 레이블)\n",
    "data_size = len(xs)\n",
    "print('말뭉치 크기: %d, 어휘 수: %d' % (corpus_size, vocab_size))\n",
    "\n",
    "# 학습 시 사용하는 변수\n",
    "max_iters = data_size // (batch_size * time_size)\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "ppl_list = []\n",
    "\n",
    "# 모델 생성\n",
    "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "\n",
    "# 미니배치의 각 샘플의 읽기 시작 위치를 계산\n",
    "jump = (corpus_size - 1) // batch_size\n",
    "offsets = [i * jump for i in range(batch_size)]\n",
    "#\n",
    "for epoch in range(max_epoch):\n",
    "    for iter in range(max_iters):\n",
    "        # 미니배치 취득\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1\n",
    "\n",
    "        # 기울기를 구하여 매개변수 갱신\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "    # 에폭마다 퍼플렉서티 평가\n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print('| 에폭 %d | 퍼플렉서티 %.2f'\n",
    "          % (epoch+1, ppl))\n",
    "    ppl_list.append(float(ppl))\n",
    "    total_loss, loss_count = 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "x = np.arange(len(ppl_list))\n",
    "plt.plot(x, ppl_list, label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('perplexity')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
